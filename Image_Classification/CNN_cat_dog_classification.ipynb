{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8bd49d40-9a36-426f-a5f0-2802650a919e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f028b267-1901-4f14-9bd9-bfcd6b990d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76b2607d-5949-4caf-af30-73a8f3275e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10274e31-f165-4769-b39e-b65988e18b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen=ImageDataGenerator(rescale=1/255,shear_range=0.2,zoom_range=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "097fcd4f-4c63-4338-b8a8-a746f8c497b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8048 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = train_datagen.flow_from_directory(\n",
    "    r'E:\\Data Science by SRK\\training_set-20241114T120946Z-001\\training_set',\n",
    "    target_size=(64, 64),\n",
    "    class_mode='binary'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38ab7915-191c-4aca-9319-3141f0faf02f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cats': 0, 'dogs': 1}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "870ff0a8-0fb3-47e2-b310-68c946dd2ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "    r'E:\\Data Science by SRK\\test_set-20241114T122521Z-001\\test_set',\n",
    "    target_size=(64, 64),\n",
    "    class_mode='binary'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05524d56-7490-4860-841c-a1a6a84136e9",
   "metadata": {},
   "source": [
    "**Modelling CNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad555020-c76c-4bb2-92fe-13b04d14629a",
   "metadata": {},
   "source": [
    "Initialising the cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14f28801-af2c-4eb5-91b7-f69a3ecf34eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "classifier=Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d223dd16-d672-47b0-aa99-9a8c79ba3877",
   "metadata": {},
   "source": [
    "**convolution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61c25b8a-13f6-4141-9046-e975bc40951d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D\n",
    "classifier.add(Conv2D(input_shape=[64,64,3],filters=32,kernel_size=3,activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f132288f-893f-44e7-bb90-f4b11b056d13",
   "metadata": {},
   "source": [
    "**Max pooling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "97045adc-6fdf-4306-9e60-65d931a61df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import MaxPooling2D\n",
    "classifier.add(MaxPooling2D(pool_size=2,strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd1eb9a-eaf5-46a0-a725-7b6bec0dc267",
   "metadata": {},
   "source": [
    "**Flattening**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0138763a-c50d-4548-b851-17f4c083fb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Flatten\n",
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb14d2a-0514-4873-97a0-c9c2d39f5ee8",
   "metadata": {},
   "source": [
    "**full connection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6801ba30-e861-40b9-8986-8cad807dc098",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "classifier.add(Dense(units=128,activation='relu'))\n",
    "classifier.add(Dense(units=1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21948f5-cd85-4e15-a642-81106f533442",
   "metadata": {},
   "source": [
    "**Training the CNN model with train data and testing tyhe model with test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "296c7645-ab6b-4860-810d-69e35558e090",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c270da2f-94d6-437b-a21b-a9571f0e477c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 346ms/step - accuracy: 0.5680 - loss: 0.8458 - val_accuracy: 0.6670 - val_loss: 0.6090\n",
      "Epoch 2/2\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 163ms/step - accuracy: 0.6976 - loss: 0.5813 - val_accuracy: 0.7200 - val_loss: 0.5503\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x26117ea8190>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(x=training_set,validation_data=test_set,epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c4724c-9e29-43f6-bf74-2e20e37444fc",
   "metadata": {},
   "source": [
    "**Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2ea5ec-b280-4fc4-ac60-7a4f399360e5",
   "metadata": {},
   "source": [
    "- Making the single prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "febca381-8a65-406f-a671-38bc7f4fe9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8895317-6cf8-45f1-b42c-c69238ce24ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder_path = r\"E:\\Data Science by SRK\\single_prediction-20241114T122744Z-001\\single_prediction\"\n",
    "print(\"Files in folder:\", os.listdir(folder_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265df7b0-ed91-41bb-acf0-e899795f8ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras models expect input in the form (batch_size, height, width, channels), but our image has (64, 64, 3) instead of (1, 64, 64, 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7a5e1077-0bda-4f66-9e67-51624b924b0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n",
      "Cat\n"
     ]
    }
   ],
   "source": [
    "image_path = r\"E:\\Data Science by SRK\\single_prediction-20241114T122744Z-001\\single_prediction\\cat_or_dog_1.jpg\"\n",
    "\n",
    "test_image = Image.open(image_path)  \n",
    "test_image = test_image.resize((64, 64))  # Resize image\n",
    "test_image = np.array(test_image)  # Convert to numpy array\n",
    "test_image = test_image / 255.0  # Normalize pixel values (important for CNN models)\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "\n",
    "#Prediction\n",
    "result=classifier.predict(test_image)\n",
    "\n",
    "#Evaluation\n",
    "if result[0][0]==1:\n",
    "    print(\"Dog\")\n",
    "else:\n",
    "    print(\"Cat\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
